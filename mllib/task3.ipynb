{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, month, year, avg, to_date\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Evapotranspiration_MLlib\").getOrCreate()\n",
        "\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/opt/data/weatherData.csv\")\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Total records: {df.count()}\")\n",
        "# df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df_clean = df.withColumnRenamed(\"precipitation_hours (h)\", \"precipitation_hours\").withColumnRenamed(\"sunshine_duration (s)\", \"sunshine_duration\") \\\n",
        "            .withColumnRenamed(\"wind_speed_10m_max (km/h)\", \"wind_speed\").withColumnRenamed(\"et0_fao_evapotranspiration (mm)\", \"evapotranspiration\")\n",
        "\n",
        "df_clean = df_clean.withColumn(\"date_parsed\", to_date(col(\"date\"), \"M/d/yyyy\")).withColumn(\"year\", year(\"date_parsed\")).withColumn(\"month\", month(\"date_parsed\"))\n",
        "\n",
        "# Filter for May (month = 5)\n",
        "df_may = df_clean.filter(col(\"month\") == 5)\n",
        "\n",
        "df_features = df_may.select(\"precipitation_hours\", \"sunshine_duration\", \"wind_speed\", \"evapotranspiration\").na.drop()\n",
        "\n",
        "print(f\"May records: {df_features.count()}\")\n",
        "df_features.describe().show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "feature_cols = [\"precipitation_hours\", \"sunshine_duration\", \"wind_speed\"]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
        "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=True)\n",
        "\n",
        "df_assembled = assembler.transform(df_features)\n",
        "scaler_model = scaler.fit(df_assembled)\n",
        "df_scaled = scaler_model.transform(df_assembled)\n",
        "\n",
        "print(\"Features assembled and scaled!\")\n",
        "df_scaled.select(\"features\", \"evapotranspiration\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Split data: 80% training, 20% validation\n",
        "train_data, test_data = df_scaled.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"Training samples: {train_data.count()}\")\n",
        "print(f\"Validation samples: {test_data.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Initialize Linear Regression\n",
        "lr = LinearRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"evapotranspiration\",\n",
        "    maxIter=100,\n",
        "    regParam=0.3,\n",
        "    elasticNetParam=0.8)\n",
        "\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "print(\"=== Linear Regression Model ===\")\n",
        "print(f\"Coefficients: {lr_model.coefficients}\")\n",
        "print(f\"Intercept: {lr_model.intercept}\")\n",
        "print(f\"R-squared (training): {lr_model.summary.r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Random Forest for comparison\n",
        "rf = RandomForestRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"evapotranspiration\",\n",
        "    numTrees=100,\n",
        "    maxDepth=10,\n",
        "    seed=42)\n",
        "\n",
        "rf_model = rf.fit(train_data)\n",
        "\n",
        "print(\"=== Random Forest Model ===\")\n",
        "print(f\"Feature Importances: {rf_model.featureImportances}\")\n",
        "print(f\"Features: {feature_cols}\")\n",
        "\n",
        "from pyspark.ml.regression import GBTRegressor # Remove this\n",
        "\n",
        "# Gradient Boosted Trees (Adding for better non-linear performance)\n",
        "gbt = GBTRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"evapotranspiration\",\n",
        "    maxIter=100,\n",
        "    seed=42)\n",
        "\n",
        "gbt_model = gbt.fit(train_data)\n",
        "print(\"\\n=== Gradient Boosted Trees (GBT) Model ===\")\n",
        "print(f\"Feature Importances: {gbt_model.featureImportances}\")\n",
        "print(f\"Features: {feature_cols}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Model evaluation\n",
        "evaluator_rmse = RegressionEvaluator(labelCol=\"evapotranspiration\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "evaluator_r2 = RegressionEvaluator(labelCol=\"evapotranspiration\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "evaluator_mae = RegressionEvaluator(labelCol=\"evapotranspiration\", predictionCol=\"prediction\", metricName=\"mae\")\n",
        "\n",
        "# Linear Regression predictions\n",
        "lr_predictions = lr_model.transform(test_data)\n",
        "lr_rmse = evaluator_rmse.evaluate(lr_predictions)\n",
        "lr_r2 = evaluator_r2.evaluate(lr_predictions)\n",
        "\n",
        "# Random Forest predictions\n",
        "rf_predictions = rf_model.transform(test_data)\n",
        "rf_rmse = evaluator_rmse.evaluate(rf_predictions)\n",
        "rf_r2 = evaluator_r2.evaluate(rf_predictions)\n",
        "\n",
        "# GBT predictions\n",
        "gbt_predictions = gbt_model.transform(test_data)\n",
        "gbt_rmse = evaluator_rmse.evaluate(gbt_predictions)\n",
        "gbt_r2 = evaluator_r2.evaluate(gbt_predictions)\n",
        "gbt_mae = evaluator_mae.evaluate(gbt_predictions)\n",
        "\n",
        "print(\"=== Linear Regression Evaluation ===\")\n",
        "print(f\"RMSE: {lr_rmse:.4f}, R2: {lr_r2:.4f}\")\n",
        "\n",
        "print(\"\\n=== Random Forest Evaluation ===\")\n",
        "print(f\"RMSE: {rf_rmse:.4f}, R2: {rf_r2:.4f}\")\n",
        "\n",
        "print(\"\\n=== Gradient Boosted Trees Evaluation ===\")\n",
        "print(f\"RMSE: {gbt_rmse:.4f}\")\n",
        "print(f\"R-2: {gbt_r2:.4f}\")\n",
        "print(f\"MAE: {gbt_mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# Find patterns where evapotranspiration < 1.5mm\n",
        "low_et = df_features.filter(col(\"evapotranspiration\") < 1.5)\n",
        "\n",
        "print(\"=== Conditions for Evapotranspiration < 1.5mm ===\")\n",
        "print(f\"Number of days with ET < 1.5mm: {low_et.count()}\")\n",
        "\n",
        "low_et_stats = low_et.agg(\n",
        "    avg(\"precipitation_hours\").alias(\"mean_precipitation_hours\"),\n",
        "    avg(\"sunshine_duration\").alias(\"mean_sunshine_duration\"),\n",
        "    avg(\"wind_speed\").alias(\"mean_wind_speed\"))\n",
        "\n",
        "print(\"\\nMean values when evapotranspiration < 1.5mm:\")\n",
        "low_et_stats.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "import random\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType\n",
        "from pyspark.sql.functions import avg, col\n",
        "\n",
        "print(\"PREDICTION: Mean Weather Conditions for Target ET < 1.5mm (GBT + Random Search)\")\n",
        "print(\"(Goal: Find average weather conditions that reliably result in < 1.5mm ET)\")\n",
        "\n",
        "# Random Grid Search Parameters\n",
        "# Based on March analysis: High Rain, Low Sun, Variable Wind\n",
        "NUM_SAMPLES = 50000\n",
        "MIN_SUNSHINE = 0.0         # Allow 0 hours\n",
        "MAX_SUNSHINE = 3600.0      # Cap at 1 hour (3600s) to force low-sunshine similarity\n",
        "MIN_WIND = 15.0            # Range around the 18.5 km/h mean\n",
        "MAX_WIND = 25.0            # Allow higher wind\n",
        "MIN_PRECIP = 20.0          # Force high rain (centered on 22h)\n",
        "MAX_PRECIP = 24.0          # Max possible\n",
        "\n",
        "print(f\"Generating {NUM_SAMPLES} synthetic realistic weather points...\")\n",
        "\n",
        "synthetic_data = []\n",
        "for _ in range(NUM_SAMPLES):\n",
        "    synthetic_data.append((\n",
        "        random.uniform(MIN_PRECIP, MAX_PRECIP),\n",
        "        random.uniform(MIN_SUNSHINE, MAX_SUNSHINE),\n",
        "        random.uniform(MIN_WIND, MAX_WIND)\n",
        "    ))\n",
        "\n",
        "# Create DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"precipitation_hours\", DoubleType(), True),\n",
        "    StructField(\"sunshine_duration\", DoubleType(), True),\n",
        "    StructField(\"wind_speed\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "df_synthetic = spark.createDataFrame(synthetic_data, schema)\n",
        "\n",
        "# Transform features\n",
        "df_synthetic_assembled = assembler.transform(df_synthetic)\n",
        "df_synthetic_scaled = scaler_model.transform(df_synthetic_assembled)\n",
        "\n",
        "# Predict using GBT\n",
        "predictions = gbt_model.transform(df_synthetic_scaled)\n",
        "\n",
        "# --- Filter for Target and Calculate Means ---\n",
        "# Keep ONLY the days where Predicted ET < 1.5\n",
        "successful_days = predictions.filter(col(\"prediction\") < 1.5)\n",
        "success_count = successful_days.count()\n",
        "\n",
        "print(f\"\\nSearch Complete.\")\n",
        "print(f\"Found {success_count} days with ET < 1.5mm out of {NUM_SAMPLES} samples.\")\n",
        "\n",
        "if success_count > 0:\n",
        "    # Calculate Mean of those successful days\n",
        "    mean_values = successful_days.agg(\n",
        "        avg(\"precipitation_hours\").alias(\"avg_precip\"),\n",
        "        avg(\"sunshine_duration\").alias(\"avg_sun\"),\n",
        "        avg(\"wind_speed\").alias(\"avg_wind\"),\n",
        "        avg(\"prediction\").alias(\"avg_et\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    recommended_stats = {\n",
        "        'precip': mean_values[\"avg_precip\"],\n",
        "        'sun': mean_values[\"avg_sun\"],\n",
        "        'wind': mean_values[\"avg_wind\"]\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n*** Recommended Mean Values for May 2026 (Target < 1.5mm) ***\")\n",
        "    print(f\"Precipitation Hours: {recommended_stats['precip']:.2f} hours\")\n",
        "    print(f\"Sunshine Duration: {recommended_stats['sun']:.2f} seconds ({recommended_stats['sun']/3600:.2f} hours)\")\n",
        "    print(f\"Wind Speed: {recommended_stats['wind']:.2f} km/h\")\n",
        "    print(f\"\\nExpected Average ET with these conditions: {mean_values['avg_et']:.4f} mm\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\nFAILURE: Could not find any weather combinations with ET < 1.5mm.\")\n",
        "    print(\"Model indicates < 1.5mm is extremely difficult/impossible within these search parameters.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": [
        "%pyspark\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "dbp-task3-2"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}